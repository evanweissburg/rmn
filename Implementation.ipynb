{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   261  100   261    0     0   2211      0 --:--:-- --:--:-- --:--:--  2211\n",
      "100   265  100   265    0     0   1409      0 --:--:-- --:--:-- --:--:--  1409\n",
      "100   244  100   244    0     0    942      0 --:--:-- --:--:-- --:--:--  7393\n",
      "100   245  100   245    0     0    587      0 --:--:-- --:--:-- --:--:--   587\n",
      "100 28.7M  100 28.7M    0     0  16.5M      0  0:00:01  0:00:01 --:--:-- 34.1M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   253  100   253    0     0   2162      0 --:--:-- --:--:-- --:--:--  2162\n",
      "100   257  100   257    0     0   1374      0 --:--:-- --:--:-- --:--:--  1374\n",
      "100   236  100   236    0     0    918      0 --:--:-- --:--:-- --:--:--   918\n",
      "100   237  100   237    0     0    573      0 --:--:-- --:--:-- --:--:--  3338\n",
      "100 1763k  100 1763k    0     0  2204k      0 --:--:-- --:--:-- --:--:-- 2204k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   249  100   249    0     0   2128      0 --:--:-- --:--:-- --:--:--  2128\n",
      "100   253  100   253    0     0   1360      0 --:--:-- --:--:-- --:--:--  1360\n",
      "100   232  100   232    0     0    902      0 --:--:-- --:--:-- --:--:--   902\n",
      "100   233  100   233    0     0    561      0 --:--:-- --:--:-- --:--:--  227k\n",
      "100 37.5M  100 37.5M    0     0  17.1M      0  0:00:02  0:00:02 --:--:-- 27.6M\n"
     ]
    }
   ],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Create directory for data and models\n",
    "!mkdir -p data\n",
    "!mkdir -p models\n",
    "!mkdir -p figs\n",
    "\n",
    "# Download data, metadata, and word embeddings\n",
    "!curl -LO http://cs.umd.edu/~miyyer/data/relationships.csv.gz\n",
    "!curl -LO http://cs.umd.edu/~miyyer/data/metadata.pkl\n",
    "!curl -LO http://cs.umd.edu/~miyyer/data/glove.We\n",
    "!mv relationships.csv.gz data/\n",
    "!mv metadata.pkl data/\n",
    "!mv glove.We data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, gzip, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(span_path, metadata_path):\n",
    "    x = csv.DictReader(gzip.open(span_path, 'rt'))\n",
    "    wmap, cmap, bmap = pickle.load(open(metadata_path, 'rb'), encoding='latin1')\n",
    "    max_len = -1\n",
    "    \n",
    "    revwmap = dict((v,k) for (k,v) in wmap.items())\n",
    "    revbmap = dict((v,k) for (k,v) in enumerate(bmap))\n",
    "    revcmap = dict((v,k) for (k,v) in cmap.items())\n",
    "    \n",
    "    span_dict = {}\n",
    "    for row in x:\n",
    "        text = row['Words'].split()\n",
    "        if len(text) > max_len:\n",
    "            max_len = len(text)\n",
    "        key = '___'.join([row['Book'], row['Char 1'], row['Char 2']])\n",
    "        if key not in span_dict:\n",
    "            span_dict[key] = []\n",
    "        span_dict[key].append([wmap[w] for w in text])\n",
    "        \n",
    "    span_data = []\n",
    "    for key in span_dict:\n",
    "        book, c1, c2 = key.split('___')\n",
    "        book = np.array([revbmap[book], ]).astype('int32')\n",
    "        chars = np.array([revcmap[c1], revcmap[c2]]).astype('int32')\n",
    "\n",
    "        # convert spans to numpy matrices \n",
    "        spans = span_dict[key]\n",
    "        s = np.zeros((len(spans), max_len)).astype('int32')\n",
    "        m = np.zeros((len(spans), max_len)).astype('float32')\n",
    "        for i in range(len(spans)):\n",
    "            curr_span = spans[i]\n",
    "            s[i][:len(curr_span)] = curr_span\n",
    "            m[i][:len(curr_span)] = 1.\n",
    "        span_data.append([book, chars, s, m])\n",
    "    return np.asarray(span_data), max_len, wmap, cmap, bmap\n",
    "\n",
    "def generate_negative_samples(num_traj, span_size, negs, span_data):\n",
    "    inds = np.random.randint(0, num_traj, negs)\n",
    "    neg_words = np.zeros((negs, span_size)).astype('int32')\n",
    "    neg_masks = np.zeros((negs, span_size)).astype('float32')\n",
    "    for index, i in enumerate(inds):\n",
    "        rand_ind = np.random.randint(0, len(span_data[i][2]))\n",
    "        neg_words[index] = span_data[i][2][rand_ind]\n",
    "        neg_masks[index] = span_data[i][3][rand_ind]\n",
    "    return torch.from_numpy(neg_words).long(), torch.from_numpy(neg_masks)\n",
    "    \n",
    "print('Loading data')\n",
    "span_data, span_size, wmap, cmap, bmap = load_data('data/relationships.csv.gz', 'data/metadata.pkl')\n",
    "We = pickle.load(open('data/glove.We', 'rb'), encoding='latin1').astype('float32')\n",
    "norm_We = We / np.linalg.norm(We, axis=1)[:, None]\n",
    "We = np.nan_to_num(norm_We)\n",
    "descriptor_log = 'models/descriptors.log'\n",
    "trajectory_log = 'models/trajectories.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding/hidden dimensionality\n",
    "d_word = We.shape[1]\n",
    "d_char = 50\n",
    "d_book = 50\n",
    "d_hidden = 50\n",
    "\n",
    "# number of descriptors\n",
    "num_descs = 30\n",
    "\n",
    "# number of negative samples per relationship\n",
    "num_negs = 50\n",
    "\n",
    "# word dropout probability\n",
    "p_drop = 0.75\n",
    "    \n",
    "n_epochs = 15\n",
    "lr = 0.001\n",
    "lda = 1e-6\n",
    "num_chars = len(cmap)\n",
    "num_books = len(bmap)\n",
    "num_traj = len(span_data)\n",
    "len_voc = len(wmap)\n",
    "revmap = {}\n",
    "for w in wmap:\n",
    "    revmap[wmap[w]] = w\n",
    "\n",
    "print('d_word: {}, span_size: {}, num_descs: {}, len_voc: {}, num_chars: {}, num_books: {}, num_traj: {}'\\\n",
    "      .format(d_word, span_size, num_descs, len_voc, num_chars, num_books, num_traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMNDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.books = [torch.as_tensor(r[0][0]) for r in data]\n",
    "        self.char1s = [torch.as_tensor(r[1][0]) for r in data]\n",
    "        self.char2s = [torch.as_tensor(r[1][1]) for r in data]\n",
    "        self.spans = [torch.from_numpy(r[2]) for r in data]\n",
    "        self.masks = [torch.from_numpy(r[3]) for r in data]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        b = self.books[index].long()\n",
    "        c1 = self.char1s[index].long()\n",
    "        c2 = self.char2s[index].long()\n",
    "        span = self.spans[index].long()\n",
    "        mask = self.masks[index]\n",
    "        return b, c1, c2, span, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RMNDataset(span_data)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=32, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, We, num_books, num_chars, d_book, d_char, d_word, num_descs, balance = 0.5):\n",
    "        super().__init__()\n",
    "        self.balance = balance\n",
    "        # embeddings\n",
    "        self.b_emb = nn.Embedding(num_books, d_book)\n",
    "        self.c_emb = nn.Embedding(num_chars, d_char)\n",
    "        self.w_emb = nn.Embedding.from_pretrained(torch.as_tensor(We, dtype=torch.float32))\n",
    "        # size d x 4d\n",
    "        self.Wbook = nn.Linear(d_book, d_word)\n",
    "        self.Wchar = nn.Linear(d_char, d_word)\n",
    "        self.Wtext = nn.Linear(d_word, d_word)\n",
    "        # size K x K + d\n",
    "        self.Wd = nn.Linear(d_word, num_descs)\n",
    "        self.Wd2 = nn.Linear(num_descs, num_descs)\n",
    "        # personality matrix\n",
    "        self.R = nn.Parameter(torch.zeros([num_descs, d_word]))\n",
    "        \n",
    "        init = [self.b_emb, self.c_emb, self.Wtext, self.Wchar, self.Wbook, self.Wd, self.Wd2]\n",
    "        for e in init:\n",
    "            torch.nn.init.xavier_uniform_(e.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.R)\n",
    "        \n",
    "    def _embed(self, book, char1, char2, span, span_mask, neg = None, neg_mask = None):\n",
    "        book = self.b_emb(book)\n",
    "        char1 = self.c_emb(char1)\n",
    "        char2 = self.c_emb(char2)\n",
    "        span = self.w_emb(span) * span_mask[:,:,None]\n",
    "        emb_span = span.sum(1).div(span_mask.sum(1, keepdim=True).clamp(1))\n",
    "        if neg is None: return book, char1, char2, emb_span\n",
    "        \n",
    "        # Add random dropout to produce new embeddings\n",
    "        drop_mask = (span_mask * torch.cuda.FloatTensor(span_mask.shape).uniform_() > p_drop).float()\n",
    "        span_drop = span * drop_mask[:,:,None]\n",
    "        emb_span_drop = span_drop.sum(1).div(drop_mask.sum(1, keepdim=True).clamp(1))\n",
    "        # Embed negative samples\n",
    "        neg = self.w_emb(neg) * neg_mask[:,:,None]\n",
    "        emb_neg = neg.sum(1).div(neg_mask.sum(1, keepdim=True).clamp(1))\n",
    "        return book, char1, char2, emb_span_drop, emb_span, emb_neg\n",
    "    \n",
    "    def _updateState(self, action, book, c1, c2, state):\n",
    "        update = F.relu(self.Wbook(book) + self.Wchar(c1) + self.Wchar(c2) + self.Wtext(action))\n",
    "        new = F.softmax(self.Wd(update) + self.Wd2(state), dim=0)\n",
    "        return self.balance * new + (1-self.balance) * state\n",
    "        \n",
    "    def forward(self, book, c1, c2, span, span_mask, neg = None, neg_mask = None):\n",
    "        if neg is not None: book, c1, c2, span_drop, span, neg = self._embed(book, c1, c2, span, span_mask, neg, neg_mask)\n",
    "        else: book, c1, c2, span = self._embed(book, char1, char2, span, span_mask)\n",
    "        # ret either contains reconstruction vectors (train) or state vectors (eval)\n",
    "        ret = []\n",
    "        state = torch.zeros(num_descs).to(device)\n",
    "        for action in span_drop if neg is not None else span:\n",
    "            state = self._updateState(action, book, c1, c2, state)\n",
    "            if neg is not None: ret.append(F.normalize(torch.matmul(self.R.t(), state), dim=0))\n",
    "            else: ret.append(state)\n",
    "        if neg is not None: return torch.stack(ret), F.normalize(span, dim=1), F.normalize(neg, dim=1)\n",
    "        else: return torch.stack(ret)\n",
    "    \n",
    "def hingeOrthoLoss(recon, truth, negs, R, I):\n",
    "    correct = torch.sum(recon * truth, axis=1)\n",
    "    incorrect = torch.matmul(recon, negs.t())\n",
    "    loss = torch.sum(torch.sum(1. - correct[:, None] + incorrect, axis=1).clamp(0))\n",
    "            \n",
    "    norm_R = F.normalize(R)\n",
    "    ortho_penalty = (torch.mm(norm_R, norm_R.t()) - I).sum()\n",
    "            \n",
    "    return loss + lda * ortho_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(We, num_books, num_chars, d_book, d_char, d_word, num_descs).to(device)\n",
    "I = torch.eye(len(net.R)).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "net.train()\n",
    "for epoch in range(n_epochs):\n",
    "    step = 0\n",
    "    cum_loss = 0\n",
    "    for book, char1, char2, span, span_mask in loader:\n",
    "        # don't use batches\n",
    "        book, char1, char2 = book[0].to(device), char1[0].to(device), char2[0].to(device)\n",
    "        span, span_mask = span[0].to(device), span_mask[0].to(device)\n",
    "\n",
    "        neg, neg_mask = generate_negative_samples(num_traj, span_size, num_negs, span_data)\n",
    "        neg, neg_mask = neg.to(device), neg_mask.to(device)\n",
    "\n",
    "        net.zero_grad()\n",
    "        recon, emb_words, emb_neg = net(book, char1, char2, span, span_mask, neg, neg_mask)\n",
    "        loss = hingeOrthoLoss(recon, emb_words, emb_neg, net.R, I)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "        cum_loss += loss\n",
    "        if step % 1000 == 0:\n",
    "            print('Epoch {}, Step {}, Loss {}'.format(epoch, step, cum_loss/1000))\n",
    "            cum_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "R = net.R.cpu().detach().numpy()\n",
    "with open(descriptor_log, 'w+') as log:\n",
    "    for ind in range(len(R)):\n",
    "        desc = R[ind] / np.linalg.norm(R[ind])\n",
    "        sims = We.dot(desc.T)\n",
    "        ordered_words = np.argsort(sims)[::-1]\n",
    "        desc_list = [revmap[w] for w in ordered_words[:10]]\n",
    "        log.write(' '.join(desc_list) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trajectory_log, 'w+') as log:\n",
    "    traj_writer = csv.writer(log)\n",
    "    traj_writer.writerow(['Book', 'Char 1', 'Char 2', 'Span ID'] + ['Topic ' + str(i) for i in range(num_descs)])\n",
    "    for book, chars, span, span_mask in span_data:\n",
    "        c1, c2 = [cmap[c] for c in chars]\n",
    "        bname = bmap[book[0]]\n",
    "        \n",
    "        book = torch.as_tensor(book[0]).long().to(device)\n",
    "        char1 = torch.as_tensor(chars[0]).long().to(device)\n",
    "        char2 = torch.as_tensor(chars[1]).long().to(device)\n",
    "        span = torch.from_numpy(span).long().to(device)\n",
    "        span_mask = torch.from_numpy(span_mask).to(device)\n",
    "\n",
    "        traj = net(book, char1, char2, span, span_mask).cpu().detach().numpy()\n",
    "        for ind in range(len(traj)):\n",
    "            step = traj[ind]\n",
    "            traj_writer.writerow([bname, c1, c2, ind] + list(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
